{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-03-26T04:11:10.344598Z","iopub.execute_input":"2025-03-26T04:11:10.34494Z","iopub.status.idle":"2025-03-26T04:11:15.736288Z","shell.execute_reply.started":"2025-03-26T04:11:10.344893Z","shell.execute_reply":"2025-03-26T04:11:15.735539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Tensorflow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:15.737714Z","iopub.execute_input":"2025-03-26T04:11:15.738208Z","iopub.status.idle":"2025-03-26T04:11:15.857918Z","shell.execute_reply.started":"2025-03-26T04:11:15.738182Z","shell.execute_reply":"2025-03-26T04:11:15.856947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download the DIV2K Dataset","metadata":{}},{"cell_type":"code","source":"# Load training and validation datasets\ntrain, train_info = tfds.load('div2k/bicubic_x8', split='train', as_supervised=True, with_info=True)\nval, val_info = tfds.load('div2k/bicubic_x8', split='validation', as_supervised=True, with_info=True)\n\ntrain_cache = train.cache()\nval_cache = val.cache()","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:15.859018Z","iopub.execute_input":"2025-03-26T04:11:15.859343Z","iopub.status.idle":"2025-03-26T04:11:16.478195Z","shell.execute_reply.started":"2025-03-26T04:11:15.859309Z","shell.execute_reply":"2025-03-26T04:11:16.477467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explore the DIV2K Dataset","metadata":{}},{"cell_type":"code","source":"# Print number of examples in the training and validation sets\nprint(f\"Number of training examples: {train_info.splits['train'].num_examples}\")\nprint(f\"Number of validation examples: {val_info.splits['validation'].num_examples}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:11:16.480138Z","iopub.execute_input":"2025-03-26T04:11:16.480409Z","iopub.status.idle":"2025-03-26T04:11:16.484944Z","shell.execute_reply.started":"2025-03-26T04:11:16.480386Z","shell.execute_reply":"2025-03-26T04:11:16.484101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Lấy 6 cặp ảnh từ tập train\nsamples = list(train_cache.take(6).as_numpy_iterator())\n\nplt.figure(figsize=(20, 38))  \n\nfor i, (lowres, highres) in enumerate(samples):\n    # Hiển thị ảnh Low-Resolution (LR)\n    ax = plt.subplot(6, 2, 2 * i + 1)\n    plt.imshow(lowres.astype(\"uint8\"))\n    plt.title(f\"LR {lowres.shape}\", fontsize=14)\n    plt.axis(\"on\")\n\n    # Hiển thị ảnh High-Resolution (HR)\n    ax = plt.subplot(6, 2, 2 * i + 2)\n    plt.imshow(highres.astype(\"uint8\"))\n    plt.title(f\"HR {highres.shape}\", fontsize=14)\n    plt.axis(\"on\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:16.485919Z","iopub.execute_input":"2025-03-26T04:11:16.486156Z","iopub.status.idle":"2025-03-26T04:11:23.562834Z","shell.execute_reply.started":"2025-03-26T04:11:16.486124Z","shell.execute_reply":"2025-03-26T04:11:23.561209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Augmentation","metadata":{}},{"cell_type":"code","source":"def flip_left_right(lowres_img, highres_img):\n    \"\"\"Flips Images to left and right.\"\"\"\n\n    rn = tf.random.uniform(shape=(), maxval=1)\n    return tf.cond(\n        rn < 0.5,\n        lambda: (lowres_img, highres_img),\n        lambda: (\n            tf.image.flip_left_right(lowres_img),\n            tf.image.flip_left_right(highres_img),\n        ),\n    )\n\n\ndef random_rotate(lowres_img, highres_img):\n    \"\"\"Rotates Images by 90 degrees.\"\"\"\n\n    rn = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n    \n    return tf.image.rot90(lowres_img, rn), tf.image.rot90(highres_img, rn)\n\n\ndef random_crop(lowres_img, highres_img, hr_crop_size=224, scale=8):\n    # Crop images.\n    \n    lowres_crop_size = hr_crop_size // scale  \n    lowres_img_shape = tf.shape(lowres_img)[:2]  \n\n    lowres_width = tf.random.uniform(\n        shape=(), maxval=lowres_img_shape[1] - lowres_crop_size + 1, dtype=tf.int32\n    )\n    lowres_height = tf.random.uniform(\n        shape=(), maxval=lowres_img_shape[0] - lowres_crop_size + 1, dtype=tf.int32\n    )\n\n    highres_width = lowres_width * scale\n    highres_height = lowres_height * scale\n\n    lowres_img_cropped = lowres_img[\n        lowres_height : lowres_height + lowres_crop_size,\n        lowres_width : lowres_width + lowres_crop_size,\n    ]  \n    highres_img_cropped = highres_img[\n        highres_height : highres_height + hr_crop_size,\n        highres_width : highres_width + hr_crop_size,\n    ]  \n\n    return lowres_img_cropped, highres_img_cropped","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:23.565001Z","iopub.execute_input":"2025-03-26T04:11:23.565314Z","iopub.status.idle":"2025-03-26T04:11:23.574429Z","shell.execute_reply.started":"2025-03-26T04:11:23.565267Z","shell.execute_reply":"2025-03-26T04:11:23.573553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create the TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"batch_size = 10\n\ndef dataset_object(dataset_cache, training=True):\n\n    ds = dataset_cache\n    ds = ds.map(\n        lambda lowres, highres: random_crop(lowres, highres, hr_crop_size = 224, scale=8),\n        num_parallel_calls=AUTOTUNE,\n    )\n\n    if training:\n        ds = ds.map(random_rotate, num_parallel_calls=AUTOTUNE)\n        ds = ds.map(flip_left_right, num_parallel_calls=AUTOTUNE)\n        \n    # Batching Data\n    ds = ds.batch(batch_size)\n\n    if training:\n        ds = ds.repeat()\n        \n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:42.071268Z","iopub.execute_input":"2025-03-26T04:11:42.071625Z","iopub.status.idle":"2025-03-26T04:11:42.07731Z","shell.execute_reply.started":"2025-03-26T04:11:42.071595Z","shell.execute_reply":"2025-03-26T04:11:42.076407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOTE: Turned off caching earlier\ntrain_ds = dataset_object(train_cache, training=True)\nval_ds = dataset_object(val_cache, training=False)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:44.645051Z","iopub.execute_input":"2025-03-26T04:11:44.6454Z","iopub.status.idle":"2025-03-26T04:11:45.034357Z","shell.execute_reply.started":"2025-03-26T04:11:44.645365Z","shell.execute_reply":"2025-03-26T04:11:45.033672Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize the Data\n\nLet's take a look at some high-res image patches, and their corresponding low-res patches.  Because we use the same figure size within matplotlib.plt, we can compare them at the same size and see how pixelated the low-res versions are.","metadata":{}},{"cell_type":"code","source":"lowres, highres = next(iter(train_ds))\n\n# High Resolution Images\nplt.figure(figsize=(20, 20))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(highres[i].numpy().astype(\"uint8\"))\n    plt.title(highres[i].shape)\n    plt.axis(\"off\")\n\n# Low Resolution Images\nplt.figure(figsize=(20, 20))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(lowres[i].numpy().astype(\"uint8\"))\n    plt.title(lowres[i].shape)\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:47.853404Z","iopub.execute_input":"2025-03-26T04:11:47.854093Z","iopub.status.idle":"2025-03-26T04:11:52.769777Z","shell.execute_reply.started":"2025-03-26T04:11:47.854064Z","shell.execute_reply":"2025-03-26T04:11:52.768816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Construct the Model","metadata":{}},{"cell_type":"code","source":"# Residual Block\ndef ResBlock(inputs):\n    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(inputs)\n    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n    x = layers.Add()([inputs, x])\n    return x\n\n# Upsampling Block\ndef Upsampling(inputs, factor=2, **kwargs):\n    x = layers.Conv2D(64 * (factor ** 2), 3, padding=\"same\", **kwargs)(inputs)\n    x = tf.nn.depth_to_space(x, block_size=factor)\n\n    # Repeat\n    x = layers.Conv2D(64 * (factor ** 2), 3, padding=\"same\", **kwargs)(x)\n    x = tf.nn.depth_to_space(x, block_size=factor)\n\n    x = layers.Conv2D(64 * (factor ** 2), 3, padding=\"same\", **kwargs)(x)\n    x = tf.nn.depth_to_space(x, block_size=factor)\n\n    return x\n\n\ndef make_model(num_filters, num_of_residual_blocks):\n    input_layer = layers.Input(shape=(None, None, 3))\n    \n    x = layers.Rescaling(scale=1.0 / 255)(input_layer)\n    \n    x = x_new = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n\n    for _ in range(num_of_residual_blocks):\n        x_new = ResBlock(x_new)\n\n    x_new = layers.Conv2D(num_filters, 3, padding=\"same\")(x_new)\n    x = layers.Add()([x, x_new])\n\n    x = Upsampling(x)\n    \n    x = layers.Conv2D(3, 3, padding=\"same\")(x)\n\n    output_layer = layers.Rescaling(scale=255)(x)\n    \n    return keras.Model(input_layer, output_layer)\n\nmodel = make_model(num_filters=64, num_of_residual_blocks=16)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:52.771449Z","iopub.execute_input":"2025-03-26T04:11:52.771722Z","iopub.status.idle":"2025-03-26T04:11:53.173004Z","shell.execute_reply.started":"2025-03-26T04:11:52.771698Z","shell.execute_reply":"2025-03-26T04:11:53.172268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Configuration","metadata":{}},{"cell_type":"code","source":"# Optimizer & Learning Rate Scheduling\noptim_edsr = keras.optimizers.Adam(learning_rate=1e-4)\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:53.965017Z","iopub.execute_input":"2025-03-26T04:11:53.965816Z","iopub.status.idle":"2025-03-26T04:11:53.974183Z","shell.execute_reply.started":"2025-03-26T04:11:53.965788Z","shell.execute_reply":"2025-03-26T04:11:53.973374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Custom PSNR Metric\n\nAs mentioned earlier, PSNR is a common metric to use for image super-resolution.  TensorFlow already offers a function under its `tf.image` module.","metadata":{}},{"cell_type":"code","source":"def PSNR(super_resolution, high_resolution):\n    \"\"\"Compute the peak signal-to-noise ratio, measures quality of image.\"\"\"\n    \n    psnr_value = tf.reduce_mean(tf.image.psnr(high_resolution, super_resolution, max_val=255))\n    return psnr_value","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:55.54522Z","iopub.execute_input":"2025-03-26T04:11:55.545536Z","iopub.status.idle":"2025-03-26T04:11:55.550013Z","shell.execute_reply.started":"2025-03-26T04:11:55.545512Z","shell.execute_reply":"2025-03-26T04:11:55.549075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Although PSNR will be our main metric, we'll use **Mean Absolute Error** (L1 Loss) as our loss function.  In theory, L2 loss (**MSE**) would minimize the PSNR, multiple papers have found empirically that using L1 loss instead results in more stable convergence and better overall results, so we'll do that here.\n\n> 🤔 **Hmmm** Remember \"compressed sensing\" that I mentioned in the beginning of this notebook?  It's all about L1 techniques.  Coincidence!?  😏","metadata":{}},{"cell_type":"code","source":"# Compiling model with loss as mean absolute error (L1 Loss) and PSNR as metric\nmodel.compile(optimizer=optim_edsr, loss=\"mae\", metrics=[PSNR])","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:57.846715Z","iopub.execute_input":"2025-03-26T04:11:57.847066Z","iopub.status.idle":"2025-03-26T04:11:57.860806Z","shell.execute_reply.started":"2025-03-26T04:11:57.847037Z","shell.execute_reply":"2025-03-26T04:11:57.859913Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checkpoint the best model\nbest_weights_checkpoint_path=\"best-model.weights.h5\"\n\nsave_best_cb = keras.callbacks.ModelCheckpoint(\n    filepath=best_weights_checkpoint_path,\n    monitor=\"val_loss\",\n    save_best_only=True,\n    save_weights_only=True,\n    save_freq=\"epoch\",\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:11:58.268419Z","iopub.execute_input":"2025-03-26T04:11:58.269265Z","iopub.status.idle":"2025-03-26T04:11:58.273834Z","shell.execute_reply.started":"2025-03-26T04:11:58.269233Z","shell.execute_reply":"2025-03-26T04:11:58.272885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train!\n\nBecause we used the `.repeat()` method on our TensorFlow dataset, the data will act like a generator and create new data infinitely.  So `.fit()` won't understand what an epoch is, since the dataset is infinite.  So, we'll need to tell it how many training steps (batches) of data we consider to be an epoch.","metadata":{}},{"cell_type":"code","source":"for lr, hr in train_ds.take(1):\n    sr = model(lr)\n    print(\"Low-Resolution Input:\", lr.shape)\n    print(\"High-Resolution Ground Truth:\", hr.shape)\n    print(\"Super-Resolution Output:\", sr.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:12:00.62605Z","iopub.execute_input":"2025-03-26T04:12:00.626623Z","iopub.status.idle":"2025-03-26T04:12:02.840742Z","shell.execute_reply.started":"2025-03-26T04:12:00.626591Z","shell.execute_reply":"2025-03-26T04:12:02.839748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_ds, \n    epochs=20,\n    steps_per_epoch=400, \n    validation_data=val_ds,\n    callbacks=[save_best_cb, lr_scheduler]\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-26T04:12:02.843431Z","iopub.execute_input":"2025-03-26T04:12:02.8437Z","iopub.status.idle":"2025-03-26T04:17:58.932815Z","shell.execute_reply.started":"2025-03-26T04:12:02.843677Z","shell.execute_reply":"2025-03-26T04:17:58.931922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Best Training Weights","metadata":{}},{"cell_type":"code","source":"model.load_weights(best_weights_checkpoint_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:17:58.936124Z","iopub.execute_input":"2025-03-26T04:17:58.93639Z","iopub.status.idle":"2025-03-26T04:17:59.19864Z","shell.execute_reply.started":"2025-03-26T04:17:58.936367Z","shell.execute_reply":"2025-03-26T04:17:59.197929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"best-model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:17:59.199951Z","iopub.execute_input":"2025-03-26T04:17:59.200283Z","iopub.status.idle":"2025-03-26T04:17:59.393744Z","shell.execute_reply.started":"2025-03-26T04:17:59.200249Z","shell.execute_reply":"2025-03-26T04:17:59.392696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_results(lowres, preds):\n    \"\"\"\n    Displays low resolution image and super resolution image side-by-side.\n    \"\"\"\n    plt.figure(figsize=(24, 14))\n    plt.subplot(132), plt.imshow(lowres), plt.title(\"Low resolution\")\n    plt.subplot(133), plt.imshow(preds), plt.title(\"Prediction\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:17:59.395647Z","iopub.execute_input":"2025-03-26T04:17:59.396022Z","iopub.status.idle":"2025-03-26T04:17:59.401394Z","shell.execute_reply.started":"2025-03-26T04:17:59.39599Z","shell.execute_reply":"2025-03-26T04:17:59.400343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Upscale Helper Function","metadata":{}},{"cell_type":"code","source":"def upscale_image(lowres):\n    \"\"\"Takes (H, W, C) image and returns (4H, 4W, C) image.\"\"\"\n    \n    model_inputs = tf.expand_dims(lowres, axis=0) \n    \n    SR = model(model_inputs, training=False)\n    \n    SR = tf.clip_by_value(SR, 0, 255)\n    SR = tf.round(SR)\n    SR = tf.cast(SR, tf.uint8)\n\n    SR = tf.squeeze(SR, axis=0)\n    \n    return SR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:18:01.270692Z","iopub.execute_input":"2025-03-26T04:18:01.271548Z","iopub.status.idle":"2025-03-26T04:18:01.276375Z","shell.execute_reply.started":"2025-03-26T04:18:01.271514Z","shell.execute_reply":"2025-03-26T04:18:01.275445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Upscale Eye Candy 😮","metadata":{}},{"cell_type":"code","source":"for lowres, _ in val.take(8):\n    \n    # lowres = tf.image.random_crop(lowres, (150, 150, 3))  \n    \n    SR = upscale_image(lowres)\n    \n    plot_results(lowres, SR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:18:10.505262Z","iopub.execute_input":"2025-03-26T04:18:10.506061Z","iopub.status.idle":"2025-03-26T04:18:23.747746Z","shell.execute_reply.started":"2025-03-26T04:18:10.506028Z","shell.execute_reply":"2025-03-26T04:18:23.746823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}